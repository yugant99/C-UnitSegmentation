# SALT Rules: Implementation Status & Impact Analysis

## üìä Current System Performance
- **Overall Accuracy**: 18.0%
- **C-Unit Accuracy**: 7.8%
- **Pause Accuracy**: 13.4%
- **Filled Pause Accuracy**: 45.5%
- **Morphological Accuracy**: 60.0%
- **Speaker Accuracy**: 50.0%

**Target**: 70%+ overall accuracy for production deployment

---

## ‚úÖ COMPLETED RULES (4/29)

### **Rule 1: Descript Input Parsing** ‚úÖ
- **Status**: COMPLETED
- **Location**: Lines 130-165 in `rule_based_processor.py`
- **Function**: `parse_descript_line_properly()`
- **Accuracy**: ~95% parsing success
- **Impact on Overall**: Foundation (no direct metric)
- **Test**: 5 test cases, 100% pass rate

### **Rule 4: C-Unit Segmentation (Core)** ‚úÖ
- **Status**: COMPLETED
- **Location**: Lines 214-302 in `rule_based_processor.py`
- **Function**: `segment_cunits_proper()`
- **Current Accuracy**: 7.8% C-unit accuracy
- **Impact on Overall**: Baseline functionality
- **Limitations**: Over-segmenting (128 system vs 105 gold C-units)
- **Test**: 5 test cases, 100% pass rate

### **Rule 11: Pause Timing Detection** ‚úÖ
- **Status**: COMPLETED
- **Location**: Lines 872-918 in `rule_based_processor.py`
- **Function**: `improve_pause_timing()`
- **Accuracy**: 13.4% pause accuracy (up from 2.1%)
- **Impact on Overall**: +32% (13.6% ‚Üí 18.0%)
- **Limitations**: Data mismatch (Descript timestamps ‚â† gold annotations)
- **Test**: 13 test cases, 100% pass rate
- **Note**: See `PAUSE_TIMING_SHORTCOMINGS.md` for detailed analysis

### **Rule 16: Filled Pause Detection** ‚úÖ
- **Status**: COMPLETED
- **Location**: Lines 304-391 in `rule_based_processor.py`
- **Function**: `detect_filled_pauses_enhanced()`
- **Accuracy**: 45.5% filled pause detection
- **Impact on Overall**: +31% (8.7% ‚Üí 11.4% in Rule 16 milestone)
- **Gold Standard**: 48 total [FP] markers across all files
- **System Output**: Detecting ~22 correctly
- **Test**: 10 test cases, 100% pass rate

---

## üéØ HIGH-IMPACT RULES (Implementable, Will Improve Accuracy)

### **Rule 2: Avatar Response Inference** üî• **TOP PRIORITY**
- **Status**: NOT IMPLEMENTED
- **Expected Impact**: +40-60% overall (18% ‚Üí 25-35%)
- **Implementation Complexity**: HIGH
- **Timeline**: 3-4 weeks

**Why This Matters:**
- System has 113 Avatar lines vs 95 in gold (VR_Django)
- System has 90 Avatar lines vs 87 in gold (VR_Howl's)
- **Missing ~20-40 Avatar responses per file that gold has**
- Missing responses create cascade of misalignments

**Gold Standard Evidence:**
```
P: Would you like breakfast?
; :02
Av: Huh.          ‚Üê Missing in Descript!
P: James, can you hear me?
```

**Implementation Plan:**
1. Build question detector (who/what/when/where/why/how/do/can/will)
2. Detect response gaps (P asks ‚Üí pause ‚Üí P continues without Av response)
3. Classify missing responses:
   - `Av: Huh.` (most common - confusion)
   - `Av: I don't understand.`
   - `Av: Okay.`
4. Insert inferred responses with pause markers
5. Handle timing alignment

**Expected Results:**
- C-Unit Accuracy: 7.8% ‚Üí 15-20%
- Pause Accuracy: 13.4% ‚Üí 25-30% (better alignment)
- Overall Accuracy: 18.0% ‚Üí 25-35%

---

### **Rule 19: Overlapping Speech Detection** üî• **HIGH PRIORITY**
- **Status**: NOT IMPLEMENTED
- **Expected Impact**: +10-15% overall (25-35% ‚Üí 35-45%)
- **Implementation Complexity**: MEDIUM-HIGH
- **Timeline**: 2-3 weeks

**Gold Standard Evidence:**
- **22 instances** of `<word>` markers across all files
- VR_Call_Me_by_Your_Name: 6 overlaps
- VR_GhostRider: 6 overlaps
- VR_Dodgeball: 5 overlaps

**Examples from Gold:**
```
P: <James>, do you want to go down for breakfast?
Av: <Huh.>

P: Or would you like to go for <coffee now>?
Av: <Coffee.>
```

**Implementation Plan:**
1. Detect timestamp overlaps in Descript input
2. Mark overlapping segments with `<word>`
3. Handle partial overlaps (word boundaries)
4. Test with gold standard examples

**Expected Results:**
- Overall Accuracy: 25-35% ‚Üí 35-40%

---

### **Rule 20: Abandoned Utterances (>)** üü° **MEDIUM PRIORITY**
- **Status**: NOT IMPLEMENTED
- **Expected Impact**: +3-5% overall
- **Implementation Complexity**: MEDIUM
- **Timeline**: 1-2 weeks

**Gold Standard Evidence:**
- **44 instances** of `>` markers across all files
- VR_Call_Me_by_Your_Name: 9 instances
- VR_Dodgeball: 8 instances

**Example from Gold:**
```
Av: There's (mm [FP]) :02 and (um [FP])> I forget.
```

**Implementation Plan:**
1. Detect incomplete thoughts (no ending punctuation)
2. Followed by speaker change or topic shift
3. Mark with `>` at end of abandoned utterance

**Expected Results:**
- Overall Accuracy: +3-5% (when combined with other rules)

---

### **Rule 28: Nonverbal Behavior Codes** üü° **MEDIUM PRIORITY**
- **Status**: NOT IMPLEMENTED
- **Expected Impact**: +5-10% overall
- **Implementation Complexity**: VERY HIGH (requires video analysis)
- **Timeline**: 4-6 weeks (if feasible)

**Gold Standard Evidence:**
- **376 instances** of `{...}` codes across all files
- VR_Django_Unchained: 79 codes
- VR_Spirited_Away: 69 codes
- VR_Get_Out: 58 codes

**Examples from Gold:**
```
{PN: looks towards bed}
{AvN: smiles}
{Av laying in bed}
{Scene transition}
```

**Implementation Challenge:**
- **Cannot be inferred from Descript text alone**
- Requires video analysis or manual annotation
- May need to skip this rule unless video access is available

**Expected Results:**
- If implementable: +5-10% overall accuracy
- If not: May need to accept lower accuracy ceiling

---

### **Rule 3: Time Marker Conversion** üü¢ **QUICK WIN**
- **Status**: PARTIALLY IMPLEMENTED
- **Expected Impact**: +2-3% overall
- **Implementation Complexity**: LOW
- **Timeline**: 1 week

**Gold Standard Evidence:**
- **94 time markers** (`-MM:SS`) across all files
- Average 9.4 markers per file

**Current Issue:**
- Function exists: `format_salt_timestamp()`
- Not integrated into pipeline

**Implementation Plan:**
1. Integrate time marker generation in `process_transcript()`
2. Convert `[00:01:30]` ‚Üí `-1:30`
3. Place at significant scene transitions

**Expected Results:**
- Overall Accuracy: +2-3%

---

## ‚ö†Ô∏è IMPLEMENTABLE BUT LOW/NO IMPACT

### **Rule 14: Morphological Marking** ‚ö†Ô∏è **SKIP**
- **Status**: PARTIALLY IMPLEMENTED (spaCy integrated)
- **Expected Impact**: 0% (gold has no morphological marks!)
- **Gold Standard Evidence**: 0 actual `/ed`, `/ing`, `/s` marks
- **Recommendation**: **SKIP** - won't improve accuracy

**Why Skip:**
- Gold standard has NO morphological marking
- SALT guide requires it, but gold wasn't annotated
- Implementing it will likely DECREASE accuracy
- 60% current accuracy is from matching "0 marks vs 0 marks"

### **Rule 27: Redaction Handling** ‚úÖ **ALREADY WORKS**
- **Status**: COMPLETED
- **Function**: `handle_redactions()` - converts `[redacted]` ‚Üí `{redacted}`
- **Impact**: Already included in baseline

### **Rule 5: Coordinating Conjunction Splitting** ‚úÖ **ALREADY WORKS**
- **Status**: COMPLETED (part of Rule 4)
- **Function**: `should_split_on_conjunction()`
- **Current Accuracy**: Working but may need refinement

### **Rule 6: Subordinating Conjunction Preservation** ‚úÖ **ALREADY WORKS**
- **Status**: COMPLETED (part of Rule 4)
- **Current Accuracy**: Working correctly

### **Rule 7: "So" vs "So That" Disambiguation** ‚úÖ **ALREADY WORKS**
- **Status**: COMPLETED (part of Rule 4)
- **Current Accuracy**: Working correctly

---

## üö´ CANNOT IMPLEMENT (Missing Data)

### **Rule 17: Maze Detection (Repetitions, False Starts)** ‚ùå
- **Status**: NOT IMPLEMENTABLE
- **Expected Impact**: Would be +5-10% if possible
- **Gold Standard Evidence**: 106 maze instances `(...)` without `[FP]`
- **Why Can't Implement**: Descript doesn't capture repetitions/false starts
- **Example Missing**:
  ```
  Gold: (I) I want to go.
  Descript: I want to go.  ‚Üê No repetition captured
  ```

### **Rule 21: Unintelligible Speech (X, XX, XXX)** ‚ùå
- **Status**: NOT IMPLEMENTABLE
- **Expected Impact**: Would be +1-2% if possible
- **Gold Standard Evidence**: 0 instances in current dataset
- **Why Can't Implement**: Descript ASR doesn't output "unintelligible"

### **Rule 22: Omissions and Partial Words (*w, *to, /*s)** ‚ùå
- **Status**: NOT IMPLEMENTABLE
- **Expected Impact**: Would be +2-5% if possible
- **Why Can't Implement**: Requires linguistic analysis of what SHOULD be said
- **Example**: Detecting missing "to" in "Give it me" ‚Üí "Give it *to me"

### **Rule 23: Linked Words (fire_truck)** ü§î **MAYBE LATER**
- **Status**: NOT IMPLEMENTED
- **Expected Impact**: +1-2%
- **Implementation Complexity**: MEDIUM
- **Why Skip for Now**: Low priority, minimal impact

### **Rule 24: Sound Effects (%woof_woof)** ‚ùå
- **Status**: NOT IMPLEMENTABLE
- **Expected Impact**: Would be +0-1% if possible
- **Gold Standard Evidence**: Rare in dataset
- **Why Can't Implement**: Descript doesn't label sound effects

### **Rule 29: Lexical Normalization (gonna, gotta, wanna)** ü§î **MAYBE LATER**
- **Status**: NOT IMPLEMENTED
- **Expected Impact**: +1-3%
- **Implementation Complexity**: LOW
- **Why Skip for Now**: Descript usually transcribes these correctly

---

## üìã RECOMMENDED IMPLEMENTATION ORDER

### **Phase 1: High-Impact Rules (6-8 weeks)**
1. ‚úÖ **Rule 2: Avatar Response Inference** (+40-60% impact)
2. ‚úÖ **Rule 3: Time Marker Integration** (+2-3% impact)
3. ‚úÖ **Rule 19: Overlapping Speech** (+10-15% impact)

**Expected Result**: 18% ‚Üí 35-45% overall accuracy

---

### **Phase 2: Medium-Impact Rules (3-4 weeks)**
4. ‚úÖ **Rule 20: Abandoned Utterances** (+3-5% impact)
5. ‚úÖ **Rule 4 Refinement: Better C-Unit Boundaries** (+5-10% impact)
   - Fix over-segmentation issue (128 system vs 105 gold)
   - Better handling of complex sentences

**Expected Result**: 35-45% ‚Üí 45-55% overall accuracy

---

### **Phase 3: Advanced Rules (4-6 weeks)**
6. ‚úÖ **Rule 28: Nonverbal Codes** (+5-10% impact IF video available)
7. ‚úÖ **Rule 8: Yes/No/Okay Responses** (+2-5% impact)
8. ‚úÖ **Rule 23: Linked Words** (+1-2% impact)
9. ‚úÖ **Rule 29: Lexical Normalization** (+1-3% impact)

**Expected Result**: 45-55% ‚Üí 55-65% overall accuracy

---

## üéØ REALISTIC ACCURACY CEILING

### **Without Video Analysis:**
- **Current**: 18.0%
- **After Phase 1**: 35-45%
- **After Phase 2**: 45-55%
- **After Phase 3**: 55-65%
- **Maximum Achievable**: **~60-65%**

### **With Video Analysis:**
- Add Rule 28 (Nonverbal Codes): +5-10%
- Add Rule 17 (Maze Detection via audio): +5-10%
- Better pause timing (human observation): +5-10%
- **Maximum Achievable**: **~75-85%**

### **Why Can't We Reach 90%+?**
1. **Missing Avatar responses** (40% of content) - Rule 2 helps but won't be perfect
2. **Descript data quality** (no repetitions, mazes, partial words)
3. **Pause timing data mismatch** (ASR timestamps ‚â† human annotations)
4. **Nonverbal codes** (require video, not just text)
5. **Subjective annotations** (human judgment varies)

---

## üìä SUMMARY TABLE

| Rule | Status | Impact | Complexity | Timeline | Priority |
|------|--------|--------|------------|----------|----------|
| **1. Input Parsing** | ‚úÖ Done | Foundation | Low | - | ‚úÖ Complete |
| **2. Avatar Response** | ‚ùå Not Done | **+40-60%** | High | 3-4 weeks | üî• TOP |
| **3. Time Markers** | üü° Partial | +2-3% | Low | 1 week | üü¢ Quick Win |
| **4. C-Unit Core** | ‚úÖ Done | Baseline | Medium | - | ‚úÖ Complete |
| **5. Coord. Conjunctions** | ‚úÖ Done | (in Rule 4) | - | - | ‚úÖ Complete |
| **6. Subord. Conjunctions** | ‚úÖ Done | (in Rule 4) | - | - | ‚úÖ Complete |
| **7. "So" Disambiguation** | ‚úÖ Done | (in Rule 4) | - | - | ‚úÖ Complete |
| **8. Yes/No Responses** | ‚ùå Not Done | +2-5% | Medium | 1-2 weeks | üü° Phase 3 |
| **11. Pause Timing** | ‚úÖ Done | +32% | Medium | - | ‚úÖ Complete |
| **14. Morphological** | ‚ö†Ô∏è Skip | 0% | Medium | - | ‚ö†Ô∏è Skip |
| **16. Filled Pauses** | ‚úÖ Done | +31% | Medium | - | ‚úÖ Complete |
| **17. Mazes** | ‚ùå Can't Do | Would be +5-10% | - | - | ‚ùå Missing Data |
| **19. Overlapping Speech** | ‚ùå Not Done | **+10-15%** | Medium | 2-3 weeks | üî• Phase 1 |
| **20. Abandoned Utterances** | ‚ùå Not Done | +3-5% | Medium | 1-2 weeks | üü° Phase 2 |
| **21. Unintelligible** | ‚ùå Can't Do | Would be +1-2% | - | - | ‚ùå Missing Data |
| **27. Redactions** | ‚úÖ Done | (in baseline) | - | - | ‚úÖ Complete |
| **28. Nonverbal Codes** | ‚ùå Not Done | +5-10%* | Very High | 4-6 weeks | üî¥ Video Req'd |
| **29. Lexical Normalization** | ‚ùå Not Done | +1-3% | Low | 1 week | üü° Phase 3 |

\* Requires video analysis

---

## üöÄ IMMEDIATE NEXT STEPS

1. **Implement Rule 2 (Avatar Response Inference)** - Biggest impact
2. **Quick Win: Rule 3 (Time Markers)** - Easy integration
3. **Then Rule 19 (Overlapping Speech)** - Clear gold evidence

**Expected Accuracy After These 3:**
- Current: 18.0%
- After Rule 2: 25-30%
- After Rule 3: 27-32%
- After Rule 19: **35-42%**

**This would nearly DOUBLE our current accuracy! üéØ**

---

**Last Updated**: After Rule 11 completion (October 2025)  
**Current System**: 4 rules implemented, 18.0% overall accuracy  
**Target**: 70%+ (realistic ceiling: 60-65% without video)

